https://medium.com/@pw33392/discover-unusual-patterns-in-time-series-data-with-unsupervised-anomaly-detection-and-isolation-78db408caaed

https://medium.com/@ketan31kumar/mastering-anomaly-detection-in-time-series-data-techniques-and-insights-98fbe94c4258

https://www.youtube.com/watch?v=qy41dXGbAxY&ab_channel=DataSciencewithMarco

----------------------------------------------
If your data has no labels indicating whether a record is anomalous or normal, you're dealing with an unsupervised anomaly detection problem. In this case, you need to rely on inherent patterns and statistical properties of the data to identify anomalies. Here’s a structured approach:

1. Preprocess the Data
Normalize/Scale: Transform the data to a uniform scale (e.g., using MinMaxScaler or StandardScaler).
Handle Missing Values: Use imputation techniques to fill gaps in the data.
Decompose Time Series (if applicable): Extract trend, seasonality, and residuals using tools like STL decomposition or Prophet.
Feature Engineering: Create meaningful features, such as rolling averages, z-scores, or Fourier transforms, to highlight anomalies.
2. Choose an Unsupervised Anomaly Detection Algorithm
Since there are no labels, the goal is to use algorithms that can detect anomalies based on deviations from learned patterns.

Options include:

Statistical Methods:

Z-Score: Flag points with z-scores beyond a threshold (e.g., ±3).
IQR (Interquartile Range): Detect outliers based on the range of Q1 - 1.5×IQR to Q3 + 1.5×IQR.
Density-Based:

DBSCAN: Identifies anomalies as low-density points.
Local Outlier Factor (LOF): Detects anomalies based on the local density of data points.
Clustering:

K-Means: Identify points far from cluster centroids as anomalies.
Gaussian Mixture Models (GMM): Anomalies are points with low likelihood under the Gaussian distribution.
Tree-Based:

Isolation Forest: Detects anomalies by isolating data points through random tree splits.
Reconstruction-Based:

PCA: Use reconstruction errors of principal components to identify anomalies.
Autoencoders: Anomalies are points with high reconstruction errors.
Time-Series Specific:

Matrix Profile: Detect discord patterns in sequences.
LSTM/GRU: Train on historical data to predict future values and identify anomalies as points with high prediction errors.
3. Tune Thresholds for Anomalies
After applying an algorithm, analyze its scores or reconstruction errors. Use methods like:
Percentile-based thresholds (e.g., top 1% of errors).
Statistical thresholds (e.g., z-score or IQR).
Domain-specific thresholds (if known).
4. Visualize and Interpret Results
Plot detected anomalies on the original data to verify visually.
Use tools like time-series plots, scatter plots, or reconstruction error graphs to understand the algorithm's performance.
5. Validate Results
Expert Feedback: Involve domain experts to validate detected anomalies and adjust parameters.
Bootstrapping/Simulations: Simulate anomalies (e.g., inject synthetic anomalies) and test the model's ability to detect them.
6. Iterate and Refine
Improve the model by tweaking parameters, choosing better features, or combining multiple methods (e.g., ensemble approaches).
Perform anomaly detection on subsets of data if the data is heterogeneous.
Key Considerations
Understand Context: Some patterns may appear as anomalies but are legitimate in the given domain.
False Positives/Negatives: Choose methods that minimize false positives for critical systems.
Scalability: Ensure the chosen approach scales well with the size of the dataset.
By using unsupervised methods, iterating based on domain knowledge, and employing validation techniques, you can effectively identify anomalies without labeled data.


-----------------------------------


-----------------------------
1. Isolation Forest
Description: Detects anomalies by isolating data points based on their path lengths in a random forest. Anomalies are easier to isolate, resulting in shorter path lengths.
Pros: Efficient with high-dimensional data and requires minimal parameter tuning.
Cons: Assumes anomalies are sparse.
Use Case: General time series with seasonal patterns and outliers.
2. Autoencoders (Deep Learning)
Description: Uses neural networks to encode and reconstruct input data. Reconstruction error is used to detect anomalies, as anomalies typically have higher reconstruction errors.
Pros: Handles complex time series patterns.
Cons: Requires substantial computational resources and training.
Use Case: Complex time series with non-linear relationships.
3. Gaussian Mixture Models (GMM)
Description: Models the data as a mixture of Gaussian distributions and identifies data points with low likelihood as anomalies.
Pros: Handles data with multiple clusters.
Cons: Sensitive to initialization and assumes Gaussian distribution.
Use Case: Time series with distinct clusters.
4. K-Means Clustering
Description: Clusters data points and flags points far from cluster centroids as anomalies.
Pros: Simple and interpretable.
Cons: Struggles with non-linear relationships and requires pre-defined cluster count.
Use Case: Simple, periodic time series data.
5. DBSCAN (Density-Based Spatial Clustering)
Description: Identifies anomalies as points in low-density regions.
Pros: Does not require specifying the number of clusters.
Cons: Sensitive to parameter tuning (epsilon and min_samples).
Use Case: Time series with varying densities.
6. One-Class SVM
Description: Trains a model to separate normal data from anomalies based on a hyperplane.
Pros: Effective for high-dimensional data.
Cons: Computationally intensive and sensitive to kernel choice.
Use Case: Time series with clear boundaries for normal behavior.
7. Dynamic Time Warping (DTW) with Clustering
Description: Measures similarity between sequences by aligning them in time. Anomalies are detected as sequences that do not fit any cluster.
Pros: Handles misaligned data and varying time series lengths.
Cons: Computationally expensive for large datasets.
Use Case: Non-periodic time series or sequences with varying lengths.
8. Prophet
Description: A forecasting model that decomposes time series into trend, seasonality, and residuals. Anomalies are identified from unexpected residuals.
Pros: Handles seasonality and missing data well.
Cons: Assumes additive decomposition.
Use Case: Seasonal time series data.
9. LSTM (Long Short-Term Memory Networks)
Description: A type of recurrent neural network (RNN) that captures long-term dependencies in sequences. Anomalies are identified based on forecasting errors.
Pros: Captures temporal dependencies effectively.
Cons: Requires large training data and computational resources.
Use Case: Time series with complex temporal dependencies.
10. Matrix Profile
Description: Identifies anomalies by computing the similarity between subsequences of a time series.
Pros: Efficient and interpretable.
Cons: May struggle with noise or irregular patterns.
Use Case: Identifying discord (anomalous subsequences) in long time series.
Recommendation
Seasonal Patterns with Outliers: Isolation Forest, Prophet
Complex Patterns: LSTM, Autoencoders
Clustering-Based: DBSCAN, K-Means
Noise-Tolerant: Matrix Profile, DTW
High Dimensionality: One-Class SVM, Isolation Forest
Choose based on the data's complexity, seasonality, and computational resources.
-------------------------------------------------------





















































from sklearn.ensemble import IsolationForest

model = IsolationForest(contamination=0.01, random_state=42)
df['anomaly_score'] = model.fit_predict(sensor_data)

df['is_anomaly'] = df['anomaly_score'] == -1

from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.5, min_samples=10)
df['cluster'] = dbscan.fit_predict(sensor_data)

# Flag anomalies as points in cluster -1
df['is_anomaly'] = df['cluster'] == -1


from sklearn.neighbors import LocalOutlierFactor

lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)
df['is_anomaly'] = lof.fit_predict(sensor_data) == -1


from sklearn.svm import OneClassSVM

model = OneClassSVM(kernel='rbf', gamma=0.1, nu=0.05)
df['is_anomaly'] = model.fit_predict(sensor_data) == -1


import matplotlib.pyplot as plt

plt.figure(figsize=(15, 5))
plt.plot(df.index, df['sensor_1'], label='Sensor 1')
plt.scatter(df.index[df['is_anomaly']], df['sensor_1'][df['is_anomaly']], color='red', label='Anomaly')
plt.legend()
plt.show()



from keras.models import Sequential
from keras.layers import Dense

# Define the Autoencoder model
model = Sequential([
    Dense(64, activation='relu', input_dim=sensor_data.shape[1]),
    Dense(32, activation='relu'),
    Dense(64, activation='relu'),
    Dense(sensor_data.shape[1], activation='linear')
])

model.compile(optimizer='adam', loss='mse')
model.fit(sensor_data, sensor_data, epochs=50, batch_size=32)

# Compute reconstruction error
reconstructions = model.predict(sensor_data)
mse = np.mean(np.power(sensor_data - reconstructions, 2), axis=1)
threshold = np.percentile(mse, 95)  # Set threshold for anomalies
df['is_anomaly'] = mse > threshold

